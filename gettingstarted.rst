==================
Getting started
==================

If the tutorial plugin has been installed, a variety of synthetic datasets will
be available in the data module. We can begin by looking at the first such
dataset, which can be imported as follows:

  >>> from pygeode.data.tutorial import t1
  >>> print t1
  <Dataset>:
  Vars:
    Temp (lat,lon)  (32,64)
  Axes:
    lat <Lat>      :  85 S to 85 N (32 values)
    lon <Lon>      :  0 E to 354 E (64 values)
  Global Attributes:
    {'history': 'Synthetic Temperature data generated by pygeode'}

We can see the dataset ``t1`` contains one pygeode variable object, ``Temp``,
which is defined on a two dimensional grid of 32 latitudes and 64 longitudes.
The grid is defined by the two pygeode axes objects, ``lat`` and ``lon``,
which span from 85 S to 85 N, and from 0 E to 354 E. respectively. 

We can take a closer look at our variable ``Temp``:

  >>> print t1.Temp
  <Var 'Temp'>:

    axes: (lat, lon)
    shape: (32, 64)

    lat <Lat>      :  85 S to 85 N (32 values)
    lon <Lon>      :  0 E to 354 E (64 values)
  Attributes:
    {'units': 'K'}

Again, we see both physical and numerical details of the grid on which this
variable is defined, as well that the temperatures are defined in degrees
Kelvin. 

Finally, we can look as well at one of our axes:

  >>> print t1.lat
  lat <Lat>      :  85 S to 85 N (32 values)

which we could also access as a member of the variable ``Temp``. 

These three types of objects, variables, axes, and datasets, are the core of
Pygeode. Variables (`:class:Var`) encapsulate a scalar field such as temperature
or one component of the wind field. Axes (`:class:Axis`), which are in fact
special cases of variables, define the geophysical grid on which the scalar
field exists. Finally, datasets (`:class:Dataset`) are containers for variables.
These concepts match reasonably closely to their counterparts in NetCDF files
(and indeed PyGeode works very naturally with NetCDF files). We'll learn more
about all three soon, but to get a quick feel of what PyGeode can do, let's try
plotting the contents of our first variable:

.. plot:: scripts/t1.py
  :include-source:

PyGeode annotates the axes automatically, and if the Basemap toolkit is
installed, data on a lat-lon grid is plotted over the outline of the continents
using a cylindrical projection. All plots generated by Pygeode are in fact
generated using the matplotlib library; the plotvar command simply makes many
educated guesses about the type of plot you might want to generate based on the
variable you've passed in. Many aspects of the plot can be customized through
the plotvar interface (which we'll get into later on in this tutorial), but if
you find you can't tweak things just so, the plot can always be manipulated
using the matplotlib library itself which gives you full control.

Now let's take a look at a second dataset:

  >>> from pygeode.data.tutorial import t2
  >>> print t2
  <Dataset>:
  Vars:
    Temp (time,pres,lat,lon)  (8760,20,32,64)
  Axes:
    time <StandardTime>:  Jan 1, 2011 00:00:00 to Dec 31, 2011 23:00:00 (8760 values)
    pres <Pres>    :  1000  hPa to 50  hPa (20 values)
    lat <Lat>      :  85 S to 85 N (32 values)
    lon <Lon>      :  0 E to 354 E (64 values)
  Global Attributes:
    {'history': 'Synthetic Temperature data generated by pygeode'}

This is a somewhat more complicated temperature field, now defined on four
dimensions: time, pressure, latitude and longitude. Note this grid has over 350
million data points--enough that the 2010-era laptop this tutorial was first
written on would complain if we attempted to load all of it into memory at once.
This brings us to one of the major guiding assumptions of pygeode - that since
we haven't done anything that needs the data itself, none of it has yet been
loaded or computed.  We can, nonetheless, manipulate this variable as if it has:

  >>> Tc = pyg.climat(t2.Temp)    # Compute the climatology
  >>> Tcz = Tc.mean('lon'))       # Compute the zonal mean
  >>> Tcp = Tc - Tcz              # Compute anomaly from the zonal mean
  >>> print Tcp

We've now computed (at least in the abstract) the climatolgical anomaly from the
zonal mean: Tcp is defined on a reduced grid (we've lost the ``Lon`` axis), and
looks (and behaves) just like any other PyGeode variable. However, no data has
yet been loaded, no actual averages have been taken, and no differences
computed, since we haven't done anything that needs the data. If we now go an
plot the near-surface anomaly of our synthetic dataset:

.. plot:: scripts/t2.py
  :include-source:

We now actually need to perform these operations (though only on one pressure
level), so PyGeode goes back and accesses the data, then carries out all of our
operations before sending the data off to matplotlib to produce a nice contour
plot. 

Implied in the operations above is another principle of PyGeode. Note that
nowhere did we need to remember which dimension of an array corresponded to 
time and which corresponded to longitude; nor did we need to know what index
described the 1000 hPa pressure level. Moreover, we could take the difference
between two fields (``Tc`` and ``Tcz``) which weren't defined on the same
grid--PyGeode takes care of the broadcasting and alignment of the underlying
numpy arrays for us, leaving us to think about the problem we're trying to solve
in the natural coordinate space of our data. The underlying mapping is of course
still there, and if it's ever more convenient to think in the space of the data
arrays, you are still free to do so. 

'This is all fine and good,' you may be thinking, 'but how do I work with my own
data?' PyGeode supports a number of data formats (NB: link?), though it perhaps
works most naturally with NetCDF (or HDF5) files. Opening a single file is as
straightforward as

  >>> from pygeode.formats import netcdf as nc
  >>> ds = nc.open('file.nc')

and a dataset (or variable) can be similarly written out: 

  >>> nc.save('file.nc', ds)

Datasets that span multiple files can be handled as well, and one can tailor
these calls so that PyGeode properly recognizes your data. More details can be
found here.
